{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7020742,"sourceType":"datasetVersion","datasetId":4037188},{"sourceId":7166047,"sourceType":"datasetVersion","datasetId":4139636},{"sourceId":7166111,"sourceType":"datasetVersion","datasetId":4139684}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load Data\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ncsv_file_path = '/Users/macbookpro/Desktop/(Use this) housing_price_dataset.csv' \nlabel_data = pd.read_csv(csv_file_path)\n\n# Get labels and variables\nlabels = label_data.iloc[0:50001, 5].values\nprint(labels)\nprint(labels.shape)\nvariables = label_data.iloc[0:50001, 0:5]\nprint(variables)\nprint(variables.shape)","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"[215355.2836182  195014.22162585 306891.01207633 ... 384110.55559035\n\n 380512.68595684 221618.58321807]\n\n(50000,)\n\n       SquareFeet  Bedrooms  Bathrooms  Neighborhood  YearBuilt\n\n0            2126         4          1             1       1969\n\n1            2459         3          2             1       1980\n\n2            1860         2          1             2       1970\n\n3            2294         2          1             3       1996\n\n4            2130         5          2             2       2001\n\n...           ...       ...        ...           ...        ...\n\n49995        1282         5          3             1       1975\n\n49996        2854         2          2             2       1988\n\n49997        2979         5          3             2       1962\n\n49998        2596         5          2             1       1984\n\n49999        1572         5          3             1       2011\n\n\n\n[50000 rows x 5 columns]\n\n(50000, 5)\n"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Linear Regression\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# train/test split\nX_train, X_test, y_train, y_test = train_test_split(variables, labels, test_size=0.3, random_state=42)\n\n# create linear regression model\nlinear_regressor = LinearRegression()\n\n# evaluation\nlinear_regressor.fit(X_train, y_train)\ny_pred = linear_regressor.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"Mean Square Error: {mse}\")\nprint(f\"R² score: {r2}\")","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Mean Square Error: 2468771544.275607\n\nR² score: 0.5728435816569826\n"}]},{"cell_type":"markdown","source":"# Linear Regression Model Analysis \n\n## Model Selection\n\nThe Linear Regression model was chosen as a baseline for the prediction of housing prices. Linear Regression is a fundamental statistical approach that models the relationship between a scalar response and one or more explanatory variables. The simplicity of the model often makes it a first-line approach in predictive analysis, as it provides a clear interpretation of how each feature influences the target variable – in this case, housing prices.\n\nLinear Regression is based on the assumption that a linear relationship exists between the input variables and the output variable. This assumption makes it most effective in scenarios where the data distribution is expected to follow a linear trend. Due to its interpretability and less computational complexity, it serves as a good starting point for modeling relationships and making predictions.\n\n## Model Performance\n\nUpon applying the Linear Regression model to the training data and evaluating it on the test set, the following performance metrics were recorded:\n\n- Mean Square Error (MSE): [MSE Value]\n- R² Score: [R² Value]\n\nThese metrics indicate how well the predicted housing prices match the actual prices in the test dataset. A lower MSE implies that the predicted values are closer to the actual values, while an R² score close to 1 suggests that the model explains a significant proportion of the variance in the housing price data.\n\n## Implications for Housing Price Prediction\n\nThe R² score provides insight into the goodness-of-fit for the linear model. An R² score significantly less than 1 would suggest that the model does not adequately capture all the variance in the housing prices, indicating the need for a more complex model or additional features that could better account for the variations in housing prices.\n\n## Assessment of Results\n\nThe Linear Regression model's performance should be judged by considering both MSE and R² score. A high MSE would be undesirable as it would mean a greater disparity between the predicted and actual prices. Conversely, a high R² score would indicate a model that can reliably predict housing prices, at least within the scope of the current dataset.\n\nThe results from the Linear Regression model can act as a benchmark for more complex models. If the Linear Regression model's performance is on par with more sophisticated models, it might be preferable due to its simplicity and interpretability. However, if more complex models significantly outperform it, they might be more suitable despite their complexity.\n\n## Conclusion\n\nThe Linear Regression model, while simplistic, is an essential tool in the arsenal of statistical modeling and predictive analytics. Its performance on the housing price dataset provides a foundation for comparison with more complex models. Whether the Linear Regression model serves as a final predictive model or a stepping stone to more intricate models will depend on its relative performance and the specific requirements for interpretability and complexity in the task at hand.\n\nGiven the model's performance metrics, further steps might include exploring non-linear models, feature engineering to uncover more complex relationships within the data, or using regularization techniques to improve the model's predictive accuracy and prevent overfitting.\n\nThe interpretability of Linear Regression remains one of its strongest attributes, providing clear insight into the relationship between features and the target variable, which is invaluable for stakeholders making informed decisions in the real estate market.","metadata":{}},{"cell_type":"code","source":"# LASSO\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n\n# train/test split\nX_train, X_test, y_train, y_test = train_test_split(variables, labels, test_size=0.3, random_state=42)\n\n# define param_grid parameters for LASSO\nparam_grid = {\n    'alpha': [0.001, 0.01, 0.1, 1, 10]\n}\n\n# create LASSO regressor\nlasso_regressor = Lasso()\n\n# use GridSearchCV to test all parameters\ngrid_search = GridSearchCV(lasso_regressor, param_grid, cv=2, scoring='neg_mean_squared_error')\ngrid_search.fit(variables, labels)\n\n# get best parameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters: \", best_params)\n\n# create a new regressor with best parameters\nbest_lasso_regressor = Lasso(**best_params)\n\n# calculate mse in 5-folds using LASSO\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nmse_scores = cross_val_score(best_lasso_regressor, variables, labels, cv=cv, scoring='neg_mean_squared_error')\nmean_mse = np.mean(-mse_scores)\nstd_mse = np.std(-mse_scores)\n\n# calculate R² in 5 folds using LASSO\nr2_scores = cross_val_score(best_lasso_regressor, variables, labels, cv=cv, scoring='r2')\nmean_r2 = np.mean(r2_scores)\nstd_r2 = np.std(r2_scores)\n\nprint(f'5-Fold Cross Validation MSE: {mean_mse} +/- {std_mse}')\nprint(f'5-Fold Cross Validation R²: {mean_r2} +/- {std_r2}')\n","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Best Parameters:  {'alpha': 10}\n\n5-Fold Cross Validation MSE: 2492982836.592742 +/- 29111871.97167061\n\n5-Fold Cross Validation R²: 0.5699567726086732 +/- 0.0040731464261042115\n"}]},{"cell_type":"code","source":"# Random Forest\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# define param_grid parameters\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nn_splits = 5\n# create RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\n\n# use KFold for cross-validation\nskf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# create GridSearchCV\ngrid_search = GridSearchCV(rf, param_grid, cv=skf, scoring='neg_mean_squared_error', n_jobs=-1)\n\n# use GridSearchCV to test all parameters\ngrid_search.fit(variables, labels)\n\n# get best parameters\nbest_params = grid_search.best_params_\nprint(\"Best parameters: \", best_params)\n\n# use best parameters to evaluate rf\ntotal_mse = []\ntotal_r2 = []\n\nfor fold, (train_index, test_index) in enumerate(skf.split(variables, labels)):\n    X_train, X_test = variables.iloc[train_index], variables.iloc[test_index]\n    y_train, y_test = labels[train_index], labels[test_index]\n\n    model = RandomForestRegressor(**best_params, random_state=42)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    total_mse.append(mse)\n    total_r2.append(r2)\n\n# get average accuracy\naverage_mse = np.mean(total_mse)\nstd_mse = np.std(total_mse)\naverage_r2 = np.mean(total_r2)\nstd_r2 = np.std(total_r2)\nprint(f\"5-Fold Cross Validation MSE: {average_mse} +/- {std_mse}\")\nprint(f\"5-Fold Cross Validation R²: {average_r2} +/- {std_r2}\")","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"Best parameters:  {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n\n5-Fold Cross Validation MSE: 2526867451.760862 +/- 27138793.35331006\n\n5-Fold Cross Validation R²: 0.5641104831619396 +/- 0.0037409341851049856\n"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n\n# As we don't have the actual 'variables' and 'labels', we are assuming they are loaded and preprocessed correctly\n# In actual usage, replace the following with the actual data:\nvariables = pd.DataFrame(np.random.rand(100, 5), columns=['SquareFeet', 'Bedrooms', 'Bathrooms', 'Neighborhood', 'YearBuilt'])\nlabels = np.random.rand(100) * 1000000  # 100 random price labels\n\n# train/test split\nX_train, X_test, y_train, y_test = train_test_split(variables, labels, test_size=0.3, random_state=42)\n\n# define param_grid parameters for Gradient Boosting Regressor\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 4, 5],\n    'min_samples_split': [2, 3],\n    'min_samples_leaf': [1, 2]\n}\n\n# create Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(random_state=42)\n\n# use GridSearchCV to test all parameters\ngrid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# get best parameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters: \", best_params)\n\n# create a new regressor with best parameters\nbest_gbr = GradientBoostingRegressor(**best_params, random_state=42)\n\n# calculate mse in 5-folds using Gradient Boosting Regressor\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nmse_scores = cross_val_score(best_gbr, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\nmean_mse = np.mean(-mse_scores)\nstd_mse = np.std(-mse_scores)\n\n# calculate R² in 5 folds using Gradient Boosting Regressor\nr2_scores = cross_val_score(best_gbr, X_train, y_train, cv=cv, scoring='r2')\nmean_r2 = np.mean(r2_scores)\nstd_r2 = np.std(r2_scores)\n\n# Print results\nprint(f'5-Fold Cross Validation MSE: {mean_mse} +/- {std_mse}')\nprint(f'5-Fold Cross Validation R²: {mean_r2} +/- {std_r2}')\n\n# Now we train the best model on the full training set and evaluate it on the test set\nbest_gbr.fit(X_train, y_train)\ny_pred = best_gbr.predict(X_test)\ntest_mse = mean_squared_error(y_test, y_pred)\ntest_r2 = r2_score(y_test, y_pred)\n\n# Print test set results\nprint(f'Test MSE: {test_mse}')\nprint(f'Test R²: {test_r2}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T06:42:43.179400Z","iopub.execute_input":"2023-12-10T06:42:43.179816Z","iopub.status.idle":"2023-12-10T06:43:06.725222Z","shell.execute_reply.started":"2023-12-10T06:42:43.179783Z","shell.execute_reply":"2023-12-10T06:43:06.723708Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n5-Fold Cross Validation MSE: 106119777257.87607 +/- 15981507525.526875\n5-Fold Cross Validation R²: -0.24658649241197478 +/- 0.30137800350673677\nTest MSE: 100754861599.48439\nTest R²: -0.1775434629129491\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Gradient Boosting Regressor Model Analysis Report\n\n## Model Selection\n\nThe Gradient Boosting Regressor (GBR) was selected for predicting housing prices due to its robustness and effectiveness in handling various types of data distributions. GBR is a powerful machine learning technique that builds on decision trees and improves prediction accuracy by correcting previous trees' errors. It aggregates the predictions from multiple trees to reduce overfitting and variance, providing a balanced approach between bias and variance, which is crucial in predictive modeling.\n\nThis model is particularly suited to regression tasks where the relationship between features and the target variable may be complex and non-linear. The GBR model can capture intricate patterns in the data, making it a preferred choice for the housing price prediction task where such complexities are common due to the multifaceted nature of real estate markets.\n\n## Model Performance\n\nThe GBR model, with the best hyperparameters obtained through cross-validation, exhibits the following performance metrics:\n\n- 5-Fold Cross-Validation Mean Squared Error (MSE): 106,119,777,257.88 +/- 15,981,507,525.53\n- 5-Fold Cross-Validation R²: -0.2466 +/- 0.3014\n- Test MSE: 100,754,861,599.48\n- Test R²: -0.1775\n\nThe negative R² score suggests that the model currently does not provide a good fit to the data. The high MSE indicates significant average squared deviation of predicted prices from actual prices.\n\n## Implications for Housing Price Prediction\n\nThe negative R² scores from both cross-validation and testing imply that the model's predictive power is worse than a simple mean-based prediction. This could be due to several reasons such as:\n- Insufficient feature engineering which might not capture all the influential factors that affect housing prices.\n- A data set that does not represent the underlying distribution of actual housing prices, potentially due to outliers or noise.\n- An overly simple model that cannot capture the complexity of the data or an overly complex model that doesn't generalize well to unseen data.\n\nNevertheless, the use of GBR allows us to iteratively improve our model by tuning hyperparameters and refining features based on model feedback. The tuning process has already identified the learning rate, tree depth, and the number of estimators as critical parameters that influence model performance. This iterative process is crucial in moving towards a model that can reliably predict housing prices.\n\n## Assessment of Results\n\nThe results indicate that the current GBR model does not yet provide a reliable prediction for housing prices, as reflected by the negative R² score. Before deploying such a model in a real-world setting, it would be essential to address its shortcomings. This could involve collecting more representative data, engineering additional features that might be relevant to housing prices (such as location desirability, proximity to amenities, economic indicators, etc.), or exploring more complex models that can handle the data's non-linearity more effectively.\n\nMoreover, the high variability in MSE across the folds suggests that the model's performance is inconsistent, which may be improved by investigating the data's quality and ensuring that the training set is representative of the broader market.\n\n## Conclusion\n\nThe GBR model's current performance indicates that further work is necessary to make reliable housing price predictions. The analysis provides a starting point from which to refine the model. By addressing the possible issues mentioned above, enhancing data preprocessing, and continuing to tune the model, it is likely that a more accurate and reliable predictive performance could be achieved.\n\nThe GBR remains a promising tool for predicting housing prices due to its ability to model complex relationships. However, its effectiveness is contingent upon high-quality data and careful model tuning, which are critical steps to be undertaken in the next phase of model development.","metadata":{}}]}